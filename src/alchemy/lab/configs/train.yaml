train:
  precision: fp32
  max_steps: 1000
  log_every_n_steps: 1

checkpoints:
  _target_: alchemy.lab.training.checkpoints.CheckpointManager
  cfg:
    _target_: alchemy.lab.training.checkpoints.CheckpointManagerConfig
    save_every_n_steps: 100
    path: "~/Projects/alchemy/src/alchemy/lab/output/weights/1"
    prefix: "unet"

dist:
  backend: nccl

model:
  _target_: alchemy.core.models.unet.unet2d.UNet2D
  cfg:
    _target_: alchemy.core.models.unet.unet2d.UNet2DConfig
    in_channels: 3
    out_channels: 3
    base_channels: 64
    channel_multipliers: [1,2,4,8]
    attn_levels: [1]
    use_mid_attn: false
    attn_num_heads: 8
    num_res_blocks: 2
    time_embed_dim: 256
    norm_groups: 32
    dropout: 0.1
    conv_bias: false

optim:
  _target_: alchemy.lab.training.optim.build_optimiser
  cfg:
    _target_: alchemy.lab.training.optim.OptimiserConfig
    name: adamw
    lr: 0.0002

loss:
  _target_: alchemy.lab.training.losses.DiffusionLossFn
  loss_cfg:
    _target_: alchemy.lab.training.losses.DiffusionLossWrapperConfig
    objective: eps
    beta_schedule_cfg:
      _target_: alchemy.core.diffusion.schedules.BetaScheduleConfig
      type: linear
      T: 1000
      beta_start: 0.0001
      beta_end: 0.02

data:
  image:
    channels: 3
    resolution: 32
  dataset:
    _target_: alchemy.lab.data.dataset.build_dataset
    cfg: {}
  loader:
    _target_: alchemy.lab.data.loader.build_dataloader
    cfg:
      _target_: alchemy.lab.data.loader.LoaderConfig
      batch_size: 128
      num_workers: 8
      shuffle: true
      drop_last: true
      pin_memory: true
      persistent_workers: true
